{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline For Disaster Response Message Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/patrick.peltier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/patrick.peltier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/patrick.peltier/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrick.peltier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import re\n",
    "\n",
    "# Imports from nltk\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Imports from  sklearn \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from database\n",
    "engine = sqlalchemy.create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql(\"select * from messages\", engine)\n",
    "\n",
    "X = df[\"message\"]\n",
    "Y = df[df.columns.difference([\"id\",\"message\",\"original\",\"genre\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"disaster_response.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Text tokenizer\n",
    "    \n",
    "        Processes text in three steps:\n",
    "        - Converts text to lower case & splits string into tokens\n",
    "        - Lemmatizes tokens\n",
    "        - Removes stopwords\n",
    "        \n",
    "        Args: \n",
    "            text (str): Input text\n",
    "        Returns:\n",
    "            str: Tokens\n",
    "    \"\"\"\n",
    "    # Remove all non-alpha-numeric characters and tokenize text\n",
    "    tokens = word_tokenize(re.sub('[^a-z0-9]', ' ', text.lower().strip()))\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok).strip() for tok in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    clean_tokens = [tok for tok in clean_tokens if tok not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Training Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(SGDClassifier(loss=\"modified_huber\", max_iter=1000, tol=1e-3),\n",
    "                                  n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...om_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False),\n",
       "           n_jobs=-1))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           aid_centers       0.33      0.01      0.02        94\n",
      "           aid_related       0.73      0.70      0.71      3600\n",
      "             buildings       0.72      0.33      0.46       427\n",
      "              clothing       0.78      0.45      0.57       134\n",
      "                  cold       0.70      0.33      0.45       180\n",
      "                 death       0.77      0.46      0.58       387\n",
      "         direct_report       0.67      0.53      0.59      1657\n",
      "            earthquake       0.89      0.77      0.82       830\n",
      "           electricity       0.63      0.25      0.36       177\n",
      "                  fire       0.85      0.25      0.39        87\n",
      "                floods       0.88      0.54      0.67       676\n",
      "                  food       0.83      0.68      0.75       996\n",
      "             hospitals       0.30      0.04      0.07        76\n",
      "infrastructure_related       0.38      0.07      0.12       539\n",
      "          medical_help       0.61      0.27      0.37       674\n",
      "      medical_products       0.72      0.29      0.41       438\n",
      "              military       0.58      0.27      0.37       274\n",
      "        missing_people       0.50      0.15      0.24        91\n",
      "                 money       0.59      0.24      0.34       184\n",
      "                 offer       0.00      0.00      0.00        42\n",
      "             other_aid       0.51      0.24      0.33      1128\n",
      "  other_infrastructure       0.29      0.04      0.08       368\n",
      "         other_weather       0.55      0.14      0.22       454\n",
      "              refugees       0.74      0.17      0.28       304\n",
      "               related       0.86      0.92      0.89      6608\n",
      "               request       0.78      0.58      0.67      1474\n",
      "     search_and_rescue       0.68      0.15      0.25       235\n",
      "              security       0.20      0.01      0.01       162\n",
      "               shelter       0.78      0.54      0.64       752\n",
      "                 shops       0.00      0.00      0.00        41\n",
      "                 storm       0.72      0.58      0.65       800\n",
      "                 tools       0.00      0.00      0.00        55\n",
      "             transport       0.73      0.22      0.33       392\n",
      "                 water       0.78      0.60      0.68       568\n",
      "       weather_related       0.83      0.72      0.77      2406\n",
      "\n",
      "           avg / total       0.74      0.60      0.65     27310\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "categories = Y_test.columns.tolist()\n",
    "print(classification_report(Y_test, Y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.30 \n",
      "Precision: 0.74 \n",
      "Recall: 0.60 \n",
      "F1 Score: 0.65 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance along accuracy, precision, recall & f1 score\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred, average=\"weighted\")\n",
    "recall = recall_score(Y_test, Y_pred, average=\"weighted\")\n",
    "f1 = f1_score(Y_test, Y_pred, average=\"weighted\")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nAccuracy: {:.2f} \\nPrecision: {:.2f} \\nRecall: {:.2f} \\nF1 Score: {:.2f} \\n\".format(\n",
    "    accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Training Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline with SGDClassifier\n",
    "random_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(SGDClassifier(loss=\"modified_huber\",\n",
    "                                                max_iter=1000,\n",
    "                                                tol=1e-3),\n",
    "                                  n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Define parameters grid\n",
    "parameters =  {\n",
    "    'clf__estimator__alpha': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "}\n",
    "\n",
    "# Create cross validation object for randomized grid search\n",
    "alpha_grid = GridSearchCV(random_pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...om_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False),\n",
       "           n_jobs=-1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'clf__estimator__alpha': [0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run grid search\n",
    "alpha_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "Y_pred = alpha_grid.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           aid_centers       0.33      0.01      0.02        94\n",
      "           aid_related       0.73      0.70      0.71      3600\n",
      "             buildings       0.72      0.33      0.46       427\n",
      "              clothing       0.78      0.45      0.57       134\n",
      "                  cold       0.70      0.33      0.45       180\n",
      "                 death       0.77      0.46      0.58       387\n",
      "         direct_report       0.67      0.53      0.59      1657\n",
      "            earthquake       0.89      0.77      0.82       830\n",
      "           electricity       0.63      0.25      0.36       177\n",
      "                  fire       0.85      0.25      0.39        87\n",
      "                floods       0.88      0.54      0.67       676\n",
      "                  food       0.83      0.68      0.75       996\n",
      "             hospitals       0.30      0.04      0.07        76\n",
      "infrastructure_related       0.38      0.07      0.12       539\n",
      "          medical_help       0.61      0.27      0.37       674\n",
      "      medical_products       0.72      0.29      0.41       438\n",
      "              military       0.58      0.27      0.37       274\n",
      "        missing_people       0.50      0.15      0.24        91\n",
      "                 money       0.59      0.24      0.34       184\n",
      "                 offer       0.00      0.00      0.00        42\n",
      "             other_aid       0.51      0.24      0.33      1128\n",
      "  other_infrastructure       0.29      0.04      0.08       368\n",
      "         other_weather       0.55      0.14      0.22       454\n",
      "              refugees       0.74      0.17      0.28       304\n",
      "               related       0.86      0.92      0.89      6608\n",
      "               request       0.78      0.58      0.67      1474\n",
      "     search_and_rescue       0.68      0.15      0.25       235\n",
      "              security       0.20      0.01      0.01       162\n",
      "               shelter       0.78      0.54      0.64       752\n",
      "                 shops       0.00      0.00      0.00        41\n",
      "                 storm       0.72      0.58      0.65       800\n",
      "                 tools       0.00      0.00      0.00        55\n",
      "             transport       0.73      0.22      0.33       392\n",
      "                 water       0.78      0.60      0.68       568\n",
      "       weather_related       0.83      0.72      0.77      2406\n",
      "\n",
      "           avg / total       0.74      0.60      0.65     27310\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "categories = Y_test.columns.tolist()\n",
    "print(classification_report(Y_test, Y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.30 \n",
      "Precision: 0.74 \n",
      "Recall: 0.60 \n",
      "F1 Score: 0.65 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance along accuracy, precision, recall & f1 score\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred, average=\"weighted\")\n",
    "recall = recall_score(Y_test, Y_pred, average=\"weighted\")\n",
    "f1 = f1_score(Y_test, Y_pred, average=\"weighted\")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nAccuracy: {:.2f} \\nPrecision: {:.2f} \\nRecall: {:.2f} \\nF1 Score: {:.2f} \\n\".format(\n",
    "    accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__alpha': 0.0001}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "alpha_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Training Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom sklearn transformer class to count number of characters\n",
    "    \"\"\"\n",
    "    def character_count(self, text):\n",
    "        \"\"\" Counts the number of characters in string\n",
    "        \n",
    "            Args: \n",
    "                text (str): Input text\n",
    "            Returns:\n",
    "                int: Number of characters\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(text)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = pd.Series(X).apply(self.character_count)\n",
    "        return pd.DataFrame(X_new.astype(str).astype(int)).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom sklearn transformer class to count number of words\n",
    "    \"\"\"\n",
    "    def word_count(self, text):\n",
    "        \"\"\" Counts the number of stopwords in string\n",
    "        \n",
    "            Args: \n",
    "                text (str): Input text\n",
    "            Returns:\n",
    "                int: Number of words\n",
    "        \"\"\"\n",
    "        tokens = nltk.word_tokenize(re.sub('[^a-z]', ' ', text.lower()))\n",
    "        return len(tokens)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = pd.Series(X).apply(self.word_count)\n",
    "        return pd.DataFrame(X_new.astype(str).astype(int)).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopwordCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom sklearn transformer class to count number of stopwords\n",
    "    \"\"\"\n",
    "    def stopword_count(self, text):\n",
    "        \"\"\" Counts the number of stopwords in string\n",
    "        \n",
    "            Args: \n",
    "                text (str): Input text\n",
    "            Returns:\n",
    "                int: Number of stopwords\n",
    "        \"\"\"\n",
    "        tokens = nltk.word_tokenize(re.sub('[^a-z]', ' ', text.lower()))\n",
    "        stopword_tokens = [tok for tok in tokens if tok in stopwords.words(\"english\")]\n",
    "        return len(stopword_tokens)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = pd.Series(X).apply(self.stopword_count)\n",
    "        return pd.DataFrame(X_new.astype(str).astype(int)).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerb(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom sklearn transformer class to check if first word is a verb\n",
    "    \"\"\"\n",
    "    def starting_verb(self, text):\n",
    "        \"\"\" Checks if first word in text is a verb\n",
    "        \n",
    "            Args: \n",
    "                text (str): Input text\n",
    "            Returns:\n",
    "                bool: True if first word is verb else False\n",
    "        \"\"\"\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            if len(pos_tags) >= 1:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new pipeline with feature unions\n",
    "random_feat_pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('nlp', Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        ('char_count', CharacterCount()),\n",
    "        ('word_count', WordCount()),\n",
    "        ('starting_verb', StartingVerb())\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(SGDClassifier(loss=\"modified_huber\", max_iter=1000, tol=1e-3, alpha=1e-4),\n",
    "                                      n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters grid\n",
    "parameters =  {\n",
    "    'features__nlp__vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'features__nlp__vect__max_df': [0.75, 1.0],\n",
    "    'features__nlp__vect__max_features': [None, 1000, 5000],\n",
    "    'features__nlp__tfidf__use_idf': [True, False],\n",
    "    'features__transformer_weights': [\n",
    "            {'nlp': 1.0,'char_count': 0.0,'word_count': 0.0,'starting_verb': 0.0},\n",
    "            {'nlp': 1.0,'char_count': 0.5,'word_count': 0.5,'starting_verb': 0.5},\n",
    "            {'nlp': 1.0,'char_count': 0.75,'word_count': 0.75,'starting_verb': 0.75},\n",
    "            {'nlp': 1.0,'char_count': 1.0,'word_count': 1.0,'starting_verb': 1.0},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create cross validation object for randomized grid search\n",
    "random_feat_grid = RandomizedSearchCV(pipeline, param_distributions=parameters, cv=3, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('nlp', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max...om_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False),\n",
       "           n_jobs=-1))]),\n",
       "          fit_params=None, iid=True, n_iter=15, n_jobs=1,\n",
       "          param_distributions={'features__nlp__vect__ngram_range': [(1, 1), (1, 2)], 'features__nlp__vect__max_df': [0.75, 1.0], 'features__nlp__vect__max_features': [None, 1000, 5000], 'features__nlp__tfidf__use_idf': [True, False], 'features__transformer_weights': [{'nlp': 1.0, 'char_count': 0.0, 'word_coun..., 'starting_verb': 0.75}, {'nlp': 1.0, 'char_count': 1.0, 'word_count': 1.0, 'starting_verb': 1.0}]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run grid search\n",
    "random_feat_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "Y_pred = random_feat_grid.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           aid_centers       0.50      0.01      0.02        94\n",
      "           aid_related       0.76      0.70      0.73      3600\n",
      "             buildings       0.67      0.39      0.49       427\n",
      "              clothing       0.77      0.49      0.60       134\n",
      "                  cold       0.60      0.31      0.41       180\n",
      "                 death       0.74      0.50      0.60       387\n",
      "         direct_report       0.70      0.47      0.56      1657\n",
      "            earthquake       0.89      0.76      0.82       830\n",
      "           electricity       0.51      0.21      0.30       177\n",
      "                  fire       0.74      0.23      0.35        87\n",
      "                floods       0.88      0.53      0.66       676\n",
      "                  food       0.82      0.67      0.74       996\n",
      "             hospitals       0.27      0.05      0.09        76\n",
      "infrastructure_related       0.42      0.05      0.09       539\n",
      "          medical_help       0.62      0.22      0.33       674\n",
      "      medical_products       0.69      0.25      0.37       438\n",
      "              military       0.61      0.26      0.37       274\n",
      "        missing_people       0.52      0.15      0.24        91\n",
      "                 money       0.60      0.30      0.40       184\n",
      "                 offer       0.00      0.00      0.00        42\n",
      "             other_aid       0.60      0.16      0.25      1128\n",
      "  other_infrastructure       0.38      0.03      0.06       368\n",
      "         other_weather       0.60      0.12      0.20       454\n",
      "              refugees       0.69      0.16      0.27       304\n",
      "               related       0.87      0.92      0.89      6608\n",
      "               request       0.75      0.58      0.66      1474\n",
      "     search_and_rescue       0.74      0.17      0.28       235\n",
      "              security       0.25      0.01      0.01       162\n",
      "               shelter       0.78      0.56      0.65       752\n",
      "                 shops       0.00      0.00      0.00        41\n",
      "                 storm       0.73      0.59      0.65       800\n",
      "                 tools       0.00      0.00      0.00        55\n",
      "             transport       0.72      0.18      0.29       392\n",
      "                 water       0.74      0.56      0.64       568\n",
      "       weather_related       0.85      0.71      0.77      2406\n",
      "\n",
      "           avg / total       0.75      0.59      0.64     27310\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "categories = Y_test.columns.tolist()\n",
    "print(classification_report(Y_test, Y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.30 \n",
      "Precision: 0.75 \n",
      "Recall: 0.59 \n",
      "F1 Score: 0.64 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/patrick.peltier/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance along accuracy, precision, recall & f1 score\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred, average=\"weighted\")\n",
    "recall = recall_score(Y_test, Y_pred, average=\"weighted\")\n",
    "f1 = f1_score(Y_test, Y_pred, average=\"weighted\")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nAccuracy: {:.2f} \\nPrecision: {:.2f} \\nRecall: {:.2f} \\nF1 Score: {:.2f} \\n\".format(\n",
    "    accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__transformer_weights': {'nlp': 1.0,\n",
       "  'char_count': 0.0,\n",
       "  'word_count': 0.0,\n",
       "  'starting_verb': 0.0},\n",
       " 'features__nlp__vect__ngram_range': (1, 1),\n",
       " 'features__nlp__vect__max_features': 1000,\n",
       " 'features__nlp__vect__max_df': 1.0,\n",
       " 'features__nlp__tfidf__use_idf': False}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "random_feat_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.pkl']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(random_feat_grid.best_estimator_, \"classifier.pkl\", compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3012805587892899"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-load from file to check if everything is correct\n",
    "joblib_model = joblib.load(\"classifier.pkl\")\n",
    "\n",
    "# Calculate the accuracy\n",
    "score = joblib_model.score(X_test, Y_test)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
